
# Parametric Continual Learning for Agents

## æ ¸å¿ƒç†å¿µ

è¿™ä¸ªæ¨¡å—å®ç°äº†**çœŸæ­£çš„Agentå±‚æŒç»­å­¦ä¹ **ï¼Œä¸ä¼ ç»Ÿçš„ICL-ERï¼ˆIn-Context Learning Experience Replayï¼‰æœ‰æœ¬è´¨åŒºåˆ«ï¼š

### ICL-ERçš„å±€é™æ€§
- âŒ ä»…å°†å†å²ç»éªŒåŠ å…¥prompt
- âŒ Agentæ²¡æœ‰å¯å­¦ä¹ å‚æ•°
- âŒ æ²¡æœ‰çœŸæ­£çš„"å­¦ä¹ "ï¼Œåªæœ‰"è®°å¿†"
- âŒ æœ¬è´¨ä¸Šæ˜¯few-shot promptingï¼Œä¸æ˜¯æŒç»­å­¦ä¹ 

### æˆ‘ä»¬çš„æ–¹æ³•
- âœ… **LLMå‚æ•°å†»ç»“** - æŒç»­å­¦ä¹ å‘ç”Ÿåœ¨Agentå±‚å’ŒMemoryå±‚
- âœ… **æ˜¾å¼å¯å­¦ä¹ å‚æ•°** - Tool Scoreræƒé‡ w_iã€Memoryé‡è¦æ€§æƒé‡ Î±_i
- âœ… **æ¢¯åº¦æ›´æ–°** - ä½¿ç”¨çœŸæ­£çš„å‚æ•°æ›´æ–°ï¼Œä¸åªæ˜¯promptå·¥ç¨‹
- âœ… **é˜²é—å¿˜æœºåˆ¶** - EWCã€Replayç­‰ç»å…¸æŒç»­å­¦ä¹ æ–¹æ³•

---

## æ¶æ„è®¾è®¡

```
Agent (Parametric)
â”œâ”€â”€ LLM (frozen) â„ï¸
â”‚   â””â”€â”€ åªç”¨äºè¯­è¨€ç”Ÿæˆå’ŒçŠ¶æ€åµŒå…¥æå–
â”‚
â”œâ”€â”€ Tool Scorer (learnable) ğŸ“Š
â”‚   â”œâ”€â”€ å‚æ•°: w_i âˆˆ R^{num_tools Ã— embedding_dim}
â”‚   â”œâ”€â”€ åŠŸèƒ½: score(s, tool_i) = w_i^T Ï†(s)
â”‚   â””â”€â”€ æ›´æ–°: æ¢¯åº¦ä¸‹é™ + å¯é€‰EWCæ­£åˆ™åŒ–
â”‚
â”œâ”€â”€ Parametric Memory (learnable) ğŸ§ 
â”‚   â”œâ”€â”€ å‚æ•°: Î±_i (importance weights)
â”‚   â”œâ”€â”€ åŠŸèƒ½: å¯å­¦ä¹ çš„ç»éªŒé‡è¦æ€§
â”‚   â””â”€â”€ æ›´æ–°: åŸºäºæ£€ç´¢æ•ˆç”¨çš„æ¢¯åº¦
â”‚
â””â”€â”€ Update Rule (method-specific) ğŸ”„
    â”œâ”€â”€ EWC: Fisher Informationæ­£åˆ™åŒ–
    â”œâ”€â”€ Replay: æ¢¯åº¦æ··åˆ
    â””â”€â”€ å…¶ä»–æ–¹æ³•...
```

---

## æ ¸å¿ƒç»„ä»¶

### 1. ToolScorer
**æ–‡ä»¶**: `tool_scorer.py`

å¯å­¦ä¹ çš„å·¥å…·é€‰æ‹©æ¨¡å—ï¼š
```python
# å·¥å…·è¯„åˆ†
score(s, tool_i) = w_i^T Ï†(s)

# å‚æ•°æ›´æ–°ï¼ˆREINFORCE-styleï¼‰
âˆ‡w_i = Î± * (reward - baseline) * âˆ‡log Ï€(tool_i | s)
```

**å…³é”®ç‰¹æ€§**ï¼š
- ä¸ºæ¯ä¸ªå·¥å…·ç»´æŠ¤æƒé‡å‘é‡ w_i
- æ”¯æŒFisher Informationè®¡ç®—ï¼ˆç”¨äºEWCï¼‰
- æ”¯æŒEWCæ­£åˆ™åŒ–æƒ©ç½š

### 2. ParametricMemory
**æ–‡ä»¶**: `parametric_memory.py`

å¸¦å¯å­¦ä¹ é‡è¦æ€§æƒé‡çš„è®°å¿†ç³»ç»Ÿï¼š
```python
# æ¯æ¡ç»éªŒæœ‰ä¸‰éƒ¨åˆ†
m_i = (z_i, Ï„_i, Î±_i)
# z_i: è½¨è¿¹åµŒå…¥ï¼ˆå›ºå®šï¼‰
# Ï„_i: æ—¶é—´æˆ³ï¼ˆå›ºå®šï¼‰
# Î±_i: é‡è¦æ€§æƒé‡ï¼ˆå¯å­¦ä¹ ï¼‰
```

**å…³é”®ç‰¹æ€§**ï¼š
- åŸºäºé‡è¦æ€§çš„é‡‡æ ·å’Œæ£€ç´¢
- é‡è¦æ€§æƒé‡çš„æ¢¯åº¦æ›´æ–°
- æ—¶é—´è¡°å‡æœºåˆ¶

### 3. ParametricCLAgent
**æ–‡ä»¶**: `base.py`

å‚æ•°åŒ–æŒç»­å­¦ä¹ AgentåŸºç±»ï¼š

**æ ¸å¿ƒæµç¨‹**ï¼š
```python
1. æå–çŠ¶æ€åµŒå…¥ Ï†(s) â† LLM (frozen)
2. å·¥å…·é€‰æ‹© â† ToolScorer (learnable)
3. æ‰§è¡ŒåŠ¨ä½œ â† Environment
4. å‚æ•°æ›´æ–° â† å­ç±»å®ç°
5. å­˜å‚¨ç»éªŒ â† ParametricMemory
```

---

## å®ç°çš„æ–¹æ³•

### æ–¹æ³•1: EWC (Elastic Weight Consolidation)
**æ–‡ä»¶**: `ewc_agent.py`

**æ ¸å¿ƒæ€æƒ³**: é€šè¿‡Fisher Informationä¿æŠ¤é‡è¦å‚æ•°

```python
# ç›®æ ‡å‡½æ•°
L = L_task + (Î»/2) * Î£_i F_i * (Î¸_i - Î¸_i*)^2

# Fisher Information
F_i = E[(âˆ‚log Ï€(a|s) / âˆ‚Î¸_i)^2]
```

**ç‰¹æ€§**ï¼š
- âœ… åœ¨çº¿EWCï¼šç´¯ç§¯å¤šä¸ªä»»åŠ¡çš„Fisher
- âœ… è‡ªé€‚åº”Î»ï¼šéšä»»åŠ¡æ•°å¢é•¿
- âœ… å‚æ•°é‡è¦æ€§åˆ†æ

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from tau2.continual_learning.agents.parametric import EWCContinualLearningAgent

agent = EWCContinualLearningAgent(
    tools=tools,
    domain_policy=policy,
    llm="gpt-4",
    embedding_dim=768,
    learning_rate=0.01,
    ewc_lambda=1.0,           # EWCå¼ºåº¦
    online_ewc=True,          # ä½¿ç”¨åœ¨çº¿EWC
    ewc_lambda_growth="adaptive",  # Î»å¢é•¿ç­–ç•¥
    fisher_sample_size=100,   # Fisherè®¡ç®—æ ·æœ¬æ•°
)

# å­¦ä¹ å¾ªç¯
for task in tasks:
    trajectory = run_task(task)
    stats = agent.learn_from_trajectory(
        task_id=task.id,
        domain=task.domain,
        trajectory=trajectory,
        reward=get_reward(trajectory),
        success=is_success(trajectory),
    )
    # statsåŒ…å«Fisherè®¡ç®—å’ŒEWCç»Ÿè®¡ä¿¡æ¯
```

### æ–¹æ³•2: Replay-based Continual Learning
**æ–‡ä»¶**: `replay_agent.py`

**æ ¸å¿ƒæ€æƒ³**: é€šè¿‡å›æ”¾æ—§ç»éªŒçš„æ¢¯åº¦æ¥é˜²æ­¢é—å¿˜

```python
# æ¢¯åº¦æ··åˆ
g_total = (1-Î±) * g_current + Î± * g_replay

# ç»éªŒæ£€ç´¢åŸºäºå¯å­¦ä¹ é‡è¦æ€§
experiences ~ ParametricMemory.sample_by_importance(Î±_i)
```

**ç‰¹æ€§**ï¼š
- âœ… å‚æ•°åŒ–ç»éªŒå›æ”¾ï¼ˆä¸åªæ˜¯promptï¼‰
- âœ… æ¢¯åº¦æ··åˆç­–ç•¥
- âœ… å¯å­¦ä¹ çš„è®°å¿†é‡è¦æ€§
- âœ… å¤šç§æ£€ç´¢ç­–ç•¥ï¼ˆé‡è¦æ€§/ç›¸ä¼¼åº¦/æ··åˆï¼‰

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from tau2.continual_learning.agents.parametric import ReplayContinualLearningAgent

agent = ReplayContinualLearningAgent(
    tools=tools,
    domain_policy=policy,
    llm="gpt-4",
    embedding_dim=768,
    learning_rate=0.01,
    replay_ratio=0.5,          # å›æ”¾æ¢¯åº¦æƒé‡
    replay_batch_size=5,       # æ¯æ¬¡å›æ”¾ç»éªŒæ•°
    replay_strategy="importance",  # æ£€ç´¢ç­–ç•¥
    update_memory_importance=True,  # æ›´æ–°é‡è¦æ€§
    replay_frequency=1,        # å›æ”¾é¢‘ç‡
)

# å­¦ä¹ å¾ªç¯
for task in tasks:
    trajectory = run_task(task)
    stats = agent.learn_from_trajectory(
        task_id=task.id,
        domain=task.domain,
        trajectory=trajectory,
        reward=get_reward(trajectory),
        success=is_success(trajectory),
    )
    # statsåŒ…å«å›æ”¾ç»Ÿè®¡å’Œé‡è¦æ€§æ›´æ–°ä¿¡æ¯
```

---

## ä¸ICL-ERçš„å¯¹æ¯”

| ç‰¹æ€§ | ICL-ER | EWC Agent | Replay Agent |
|-----|--------|-----------|--------------|
| **å¯å­¦ä¹ å‚æ•°** | âŒ æ—  | âœ… w_i, Î±_i | âœ… w_i, Î±_i |
| **å‚æ•°æ›´æ–°** | âŒ æ—  | âœ… æ¢¯åº¦+EWC | âœ… æ¢¯åº¦+Replay |
| **é˜²é—å¿˜æœºåˆ¶** | âŒ æ—  | âœ… Fisheræ­£åˆ™åŒ– | âœ… ç»éªŒå›æ”¾ |
| **çœŸæ­£çš„å­¦ä¹ ** | âŒ åªæ˜¯è®°å¿† | âœ… æ˜¯ | âœ… æ˜¯ |
| **å·¥å…·é€‰æ‹©** | LLMå†³å®š | Scorerå†³å®š | Scorerå†³å®š |
| **è®°å¿†ç®¡ç†** | å›ºå®šé‡‡æ · | å¯å­¦ä¹ é‡è¦æ€§ | å¯å­¦ä¹ é‡è¦æ€§ |

---

## å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
from tau2.continual_learning.agents.parametric import (
    EWCContinualLearningAgent,
    ReplayContinualLearningAgent,
)
from tau2.continual_learning.orchestrator import CLOrchestrator
from tau2.environment.tool import Tool

# 1. å®šä¹‰å·¥å…·
tools = [
    Tool.from_function(search_database),
    Tool.from_function(send_email),
    Tool.from_function(create_ticket),
]

# 2. åˆ›å»ºAgent
agent = EWCContinualLearningAgent(
    tools=tools,
    domain_policy=load_policy("customer_service"),
    llm="gpt-4",
    embedding_dim=768,
    learning_rate=0.01,
    ewc_lambda=1.0,
    online_ewc=True,
)

# 3. åˆ›å»ºæŒç»­å­¦ä¹ Orchestrator
orchestrator = CLOrchestrator(
    agent=agent,
    curriculum=SequentialCurriculum(domains=["domain1", "domain2", "domain3"]),
    eval_frequency=10,
    save_checkpoints=True,
)

# 4. è¿è¡ŒæŒç»­å­¦ä¹ å®éªŒ
results = orchestrator.run(
    num_tasks_per_domain=50,
    eval_on_all_domains=True,
)

# 5. åˆ†æç»“æœ
print("å¹³å‡æ€§èƒ½:", results["avg_performance"])
print("é—å¿˜åº¦:", results["forgetting"])
print("å‰å‘è¿ç§»:", results["forward_transfer"])

# 6. å¯è§†åŒ–
import matplotlib.pyplot as plt

plt.plot(results["performance_over_time"])
plt.xlabel("Task")
plt.ylabel("Performance")
plt.title("Continual Learning Performance")
plt.show()

# 7. ä¿å­˜Agent
agent.save_state("checkpoints/agent_final.json")

# 8. åŠ è½½Agent
agent.load_state("checkpoints/agent_final.json")
```

---

## çŠ¶æ€åµŒå…¥æå–

**å…³é”®é—®é¢˜**: å¦‚ä½•ä»å†»ç»“çš„LLMä¸­æå–çŠ¶æ€åµŒå…¥ Ï†(s)ï¼Ÿ

**å½“å‰å®ç°** (`base.py:_extract_state_embedding`):
```python
def _extract_state_embedding(self, messages):
    # æ–¹æ³•1: ä½¿ç”¨OpenAI Embedding API
    text = extract_text(messages)
    embedding = openai_client.embeddings.create(
        model="text-embedding-3-small",
        input=text,
        dimensions=768,
    )
    return embedding

    # æ–¹æ³•2: æå–LLMéšè—å±‚ï¼ˆéœ€è¦æ¨¡å‹è®¿é—®ï¼‰
    # hidden_states = llm.get_hidden_states(messages)
    # return hidden_states[-1].mean(dim=1)
```

**æ”¹è¿›æ–¹å‘**ï¼š
- ä½¿ç”¨æ¨¡å‹çš„å®é™…hidden statesï¼ˆå¦‚æœå¯è®¿é—®ï¼‰
- ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–åµŒå…¥ç©ºé—´
- ä»»åŠ¡ç‰¹å®šçš„åµŒå…¥é€‚é…å™¨

---

## è¯„ä¼°æŒ‡æ ‡

### 1. å¹³å‡æ€§èƒ½ (Average Performance)
æ‰€æœ‰ä»»åŠ¡çš„å¹³å‡æˆåŠŸç‡

### 2. é—å¿˜åº¦ (Forgetting)
```
F = (1/T) * Î£_t max_k(P_{t,k} - P_{T,k})
```
å…¶ä¸­ P_{t,k} æ˜¯åœ¨å­¦ä¹ ä»»åŠ¡tåï¼Œåœ¨ä»»åŠ¡kä¸Šçš„æ€§èƒ½

### 3. å‰å‘è¿ç§» (Forward Transfer)
```
FWT = (1/T) * Î£_t (P_{t,t} - P_{0,t})
```
å­¦ä¹ æ–°ä»»åŠ¡æ—¶ç›¸æ¯”éšæœºåˆå§‹åŒ–çš„æå‡

### 4. å‚æ•°å˜åŒ–åˆ†æ
- Fisher Informationçš„åˆ†å¸ƒ
- å‚æ•°æ›´æ–°çš„å¹…åº¦
- é‡è¦æ€§æƒé‡çš„æ¼”åŒ–

---

## å®éªŒå»ºè®®

### 1. Baselineå¯¹æ¯”
- **No-CL**: æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹è®­ç»ƒ
- **Joint**: æ‰€æœ‰ä»»åŠ¡è”åˆè®­ç»ƒï¼ˆä¸Šç•Œï¼‰
- **ICL-ER**: åŸå§‹çš„prompt-basedæ–¹æ³•
- **EWC**: æˆ‘ä»¬çš„EWC Agent
- **Replay**: æˆ‘ä»¬çš„Replay Agent

### 2. æ¶ˆèå®éªŒ
- Tool Scorerçš„å½±å“
- Memory importanceçš„å½±å“
- ä¸åŒÎ»å€¼çš„å½±å“
- ä¸åŒreplay ratioçš„å½±å“

### 3. ä»»åŠ¡åºåˆ—
- Sequential: é¡ºåºå­¦ä¹ ï¼ˆæœ€éš¾ï¼‰
- Interleaved: äº¤é”™å­¦ä¹ 
- Curriculum: ç”±æ˜“åˆ°éš¾

---

## æ–‡ä»¶ç»“æ„

```
parametric/
â”œâ”€â”€ __init__.py              # æ¨¡å—å…¥å£
â”œâ”€â”€ tool_scorer.py           # å¯å­¦ä¹ å·¥å…·é€‰æ‹©å™¨
â”œâ”€â”€ parametric_memory.py     # å¯å­¦ä¹ è®°å¿†ç³»ç»Ÿ
â”œâ”€â”€ base.py                  # å‚æ•°åŒ–CL AgentåŸºç±»
â”œâ”€â”€ ewc_agent.py            # EWCæ–¹æ³•å®ç°
â”œâ”€â”€ replay_agent.py         # Replayæ–¹æ³•å®ç°
â”œâ”€â”€ README.md               # æœ¬æ–‡æ¡£
â””â”€â”€ example_usage.py        # ä½¿ç”¨ç¤ºä¾‹
```

---

## æœªæ¥æ‰©å±•

### å…¶ä»–æŒç»­å­¦ä¹ æ–¹æ³•
- [ ] **Progressive Networks**: ä¸ºæ–°ä»»åŠ¡æ·»åŠ æ–°æ¨¡å—
- [ ] **PackNet**: å‚æ•°åˆ†é…å’Œæ‰“åŒ…
- [ ] **Meta-CL**: å­¦ä¹ å¦‚ä½•å­¦ä¹ çš„å…ƒå‚æ•°
- [ ] **Parameter Isolation**: ä»»åŠ¡ç‰¹å®šå‚æ•°å­é›†

### æ”¹è¿›æ–¹å‘
- [ ] æ›´å¥½çš„çŠ¶æ€åµŒå…¥æå–æ–¹æ³•
- [ ] è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
- [ ] å¤šæ¨¡æ€çŠ¶æ€è¡¨ç¤º
- [ ] åˆ†å±‚è®°å¿†ç³»ç»Ÿ

---

## å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬ä»£ç ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@inproceedings{tau2-parametric-cl,
  title={Parametric Continual Learning for Tool-using Agents},
  author={Your Name},
  booktitle={Proceedings of ...},
  year={2025}
}
```

---

## æ ¸å¿ƒåˆ›æ–°æ€»ç»“

1. **é¦–æ¬¡**å°†ç»å…¸æŒç»­å­¦ä¹ æ–¹æ³•ç³»ç»Ÿåœ°æ˜ å°„åˆ°Agent Tool-useå±‚
2. **æ˜ç¡®**åŒºåˆ†äº†LLMå±‚ï¼ˆå†»ç»“ï¼‰å’ŒAgentå±‚ï¼ˆå¯å­¦ä¹ ï¼‰
3. **å¼•å…¥**æ˜¾å¼å¯å­¦ä¹ å‚æ•°ï¼ˆTool Scoreræƒé‡ã€Memoryé‡è¦æ€§ï¼‰
4. **å®ç°**çœŸæ­£çš„æ¢¯åº¦æ›´æ–°ï¼Œè€Œéä»…ä»…promptå·¥ç¨‹
5. **æä¾›**ç»Ÿä¸€æ¡†æ¶ï¼Œä¾¿äºå…¬å¹³æ¯”è¾ƒä¸åŒCLæ–¹æ³•

è¿™æ˜¯çœŸæ­£çš„**Agent-level Continual Learning**ï¼Œè€Œä¸æ˜¯ä¼ªè£…çš„few-shot learningï¼
