
# å‚æ•°åŒ–æŒç»­å­¦ä¹  - è¿è¡ŒæŒ‡å—

## ğŸ“– ä¸‰ç§è¿è¡Œæ–¹å¼

### æ–¹å¼1: æœ€ç®€å• - ç›´æ¥å¯¼å…¥ä½¿ç”¨ï¼ˆæ¨èï¼‰

åœ¨ä½ çš„Pythonè„šæœ¬ä¸­ï¼š

```python
from tau2.continual_learning.agents.parametric import EWCContinualLearningAgent
from tau2.environment.tool import Tool

# å®šä¹‰å·¥å…·
def my_tool(query: str) -> str:
    """æˆ‘çš„å·¥å…·"""
    return f"ç»“æœ: {query}"

# åˆ›å»ºagent
agent = EWCContinualLearningAgent(
    tools=[Tool(my_tool)],
    domain_policy="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹",
    llm="gpt-4",
)

# ä½¿ç”¨agentï¼ˆéœ€è¦é…åˆtau2çš„orchestratorï¼‰
# æˆ–è€…ç›´æ¥è°ƒç”¨learn_from_trajectory
```

### æ–¹å¼2: é›†æˆåˆ°ç°æœ‰tau2é¡¹ç›®

å¦‚æœä½ å·²ç»æœ‰tau2çš„æŒç»­å­¦ä¹ å®éªŒï¼š

```python
# åœ¨ä½ çš„å®éªŒè„šæœ¬ä¸­
from tau2.continual_learning.agents.parametric import (
    EWCContinualLearningAgent,
    ReplayContinualLearningAgent,
    # ... å…¶ä»–æ–¹æ³•
)

# æ›¿æ¢åŸæ¥çš„ICL-ER Agent
# agent = ICLExperienceReplayAgent(...)  # æ—§æ–¹æ³•
agent = EWCContinualLearningAgent(...)   # æ–°æ–¹æ³•ï¼ˆå¸¦å‚æ•°å­¦ä¹ ï¼‰

# å…¶ä½™ä»£ç ä¿æŒä¸å˜
for task in task_stream:
    trajectory = orchestrator.run(agent, task)

    # å…³é”®ï¼šè®©agentå­¦ä¹ ï¼ˆå‚æ•°æ›´æ–°ï¼‰
    agent.learn_from_trajectory(
        task_id=task.id,
        domain=task.domain,
        trajectory=trajectory.messages,
        reward=compute_reward(trajectory),
        success=is_success(trajectory),
    )
```

### æ–¹å¼3: ç‹¬ç«‹æµ‹è¯•ï¼ˆä¸éœ€è¦å®Œæ•´ç¯å¢ƒï¼‰

åˆ›å»º`test_parametric_agent.py`:

```python
import numpy as np
from tau2.continual_learning.agents.parametric import EWCContinualLearningAgent
from tau2.continual_learning.agents.parametric.tool_scorer import ToolScorer
from tau2.environment.tool import Tool

# å®šä¹‰ç®€å•å·¥å…·
def search(query: str) -> str:
    """æœç´¢å·¥å…·"""
    return f"Found: {query}"

# åˆ›å»ºagent
tools = [Tool(search)]
policy = "Be helpful"

agent = EWCContinualLearningAgent(
    tools=tools,
    domain_policy=policy,
    llm="gpt-4",
    embedding_dim=768,
    learning_rate=0.01,
    ewc_lambda=1.0,
)

print(f"âœ“ Agentåˆ›å»ºæˆåŠŸ: {agent.__class__.__name__}")
print(f"âœ“ å·¥å…·æ•°é‡: {len(agent.tools)}")
print(f"âœ“ Tool Scorerå‚æ•°shape: {agent.tool_scorer.weights.shape}")

# æµ‹è¯•å‚æ•°æ›´æ–°
state_emb = np.random.randn(768)
selected_tool = "search"
reward = 1.0

update_stats = agent._update_parameters(
    state_embedding=state_emb,
    selected_tool=selected_tool,
    reward=reward,
    success=True,
)

print(f"âœ“ å‚æ•°æ›´æ–°æˆåŠŸ: {update_stats['updated']}")
print(f"  - å·¥å…·: {update_stats['tool']}")
print(f"  - å¥–åŠ±: {update_stats['reward']}")
print(f"  - æ¦‚ç‡: {update_stats['probability']:.4f}")

print("\nğŸ‰ æµ‹è¯•é€šè¿‡ï¼Agentçš„å‚æ•°ç¡®å®å¯ä»¥å­¦ä¹ å’Œæ›´æ–°ï¼")
```

è¿è¡Œï¼š
```bash
python test_parametric_agent.py
```

## ğŸš€ å®Œæ•´ç¤ºä¾‹ï¼ˆéœ€è¦tau2ç¯å¢ƒï¼‰

å¦‚æœä½ æœ‰å®Œæ•´çš„tau2-benchç¯å¢ƒï¼Œå¯ä»¥è¿è¡Œï¼š

```bash
# è¿è¡Œå®Œæ•´çš„CLå®éªŒ
python src/tau2/run.py \
    --agent ewc \
    --domains customer_service,tech_support \
    --num_tasks 50

# æˆ–ä½¿ç”¨CLOrchestrator
python -c "
from tau2.continual_learning.orchestrator import CLOrchestrator
from tau2.continual_learning.agents.parametric import EWCContinualLearningAgent

# åˆ›å»ºagent
agent = EWCContinualLearningAgent(...)

# åˆ›å»ºorchestrator
orchestrator = CLOrchestrator(
    agent=agent,
    curriculum=your_curriculum,
)

# è¿è¡Œå®éªŒ
results = orchestrator.run()
print(f'å¹³å‡æ€§èƒ½: {results[\"avg_performance\"]:.3f}')
print(f'é—å¿˜åº¦: {results[\"forgetting\"]:.3f}')
"
```

## ğŸ“Š æ–¹æ³•å¯¹æ¯”æµ‹è¯•

æµ‹è¯•5ç§æ–¹æ³•çš„æ€§èƒ½ï¼š

```python
from tau2.continual_learning.agents.parametric import (
    EWCContinualLearningAgent,
    ReplayContinualLearningAgent,
    ParameterIsolationAgent,
    ProgressiveModularAgent,
    MetaContinualLearningAgent,
)

methods = {
    "EWC": EWCContinualLearningAgent,
    "Replay": ReplayContinualLearningAgent,
    "ParamIso": ParameterIsolationAgent,
    "Progressive": ProgressiveModularAgent,
    "MetaCL": MetaContinualLearningAgent,
}

results = {}
for name, AgentClass in methods.items():
    agent = AgentClass(
        tools=tools,
        domain_policy=policy,
        llm="gpt-4",
    )

    # è¿è¡Œå®éªŒ
    perf = run_experiment(agent, tasks)
    results[name] = perf

    print(f"{name}: {perf['accuracy']:.3f}, é—å¿˜={perf['forgetting']:.3f}")
```

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: éœ€è¦ä»€ä¹ˆç¯å¢ƒï¼Ÿ
A: éœ€è¦tau2-benchç¯å¢ƒã€‚å¦‚æœåªæ˜¯æµ‹è¯•å‚æ•°æ›´æ–°ï¼Œåªéœ€è¦numpyå’ŒåŸºç¡€ä¾èµ–ã€‚

### Q2: å¦‚ä½•é€‰æ‹©æ–¹æ³•ï¼Ÿ
A:
- ä»»åŠ¡ç›¸ä¼¼ â†’ EWCæˆ–Replay
- ä»»åŠ¡å·®å¼‚å¤§ â†’ Parameter Isolationæˆ–Progressive
- ç»ˆèº«å­¦ä¹  â†’ Progressiveæˆ–Meta-CL
- è¯¦è§ `METHODS_COMPARISON.md`

### Q3: ä¸ICL-ERçš„åŒºåˆ«ï¼Ÿ
A: ICL-ERåªæ˜¯æŠŠç»éªŒåŠ åˆ°promptï¼Œæ²¡æœ‰å‚æ•°å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰çœŸæ­£çš„å¯å­¦ä¹ å‚æ•°å’Œæ¢¯åº¦æ›´æ–°ã€‚è¯¦è§ `COMPARISON.md`

### Q4: å¦‚ä½•æŸ¥çœ‹å­¦ä¹ æ•ˆæœï¼Ÿ
A:
```python
# æŸ¥çœ‹ç»Ÿè®¡
stats = agent.get_statistics()
print(stats['num_tasks_learned'])
print(stats['tool_scorer_stats'])

# æŸ¥çœ‹Fisher Information (EWC)
if 'cumulative_fisher_stats' in stats:
    print(stats['cumulative_fisher_stats'])

# æŸ¥çœ‹å‚æ•°å˜åŒ–
params = agent.get_parameters()
print(params['tool_scorer']['weights'].shape)
```

## ğŸ“š æ–‡æ¡£

- `README.md` - å®Œæ•´ä½¿ç”¨æŒ‡å—
- `COMPARISON.md` - ä¸ICL-ERçš„è¯¦ç»†å¯¹æ¯”
- `METHODS_COMPARISON.md` - 5ç§æ–¹æ³•å…¨é¢å¯¹æ¯”
- `FINAL_SUMMARY.md` - é¡¹ç›®æ€»ç»“

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

```python
# æ—§æ–¹æ³• (ICL-ER) - æ— å‚æ•°å­¦ä¹ 
agent.memory.append(experience)  # åªå­˜å‚¨
response = llm(prompt)            # LLMå†³å®š

# æ–°æ–¹æ³• (Parametric) - çœŸæ­£å­¦ä¹ 
gradient = compute_gradient(reward)  # è®¡ç®—æ¢¯åº¦
w += lr * gradient                   # æ›´æ–°å‚æ•°
# Agentå­¦åˆ°äº†ï¼
```

## âœ¨ å…³é”®ä¼˜åŠ¿

- âœ… çœŸæ­£çš„å‚æ•°å­¦ä¹ ï¼ˆä¸åªæ˜¯promptï¼‰
- âœ… 5ç§ç»å…¸CLæ–¹æ³•
- âœ… é˜²é—å¿˜æœºåˆ¶
- âœ… ç»Ÿä¸€æ¥å£
- âœ… å®Œæ•´æ–‡æ¡£

å¼€å§‹ä½¿ç”¨å§ï¼ğŸš€
