#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„å‚æ•°åŒ–æŒç»­å­¦ä¹ æ–¹æ³•å¯¹æ¯”å®éªŒ

å¯¹æ¯”5ç§æ–¹æ³•ï¼š
1. ICL-ER (Baseline) - éå‚æ•°åŒ–
2. EWC - Elastic Weight Consolidation
3. Replay - Experience Replay
4. Parameter Isolation - å‚æ•°éš”ç¦»
5. Meta-CL - å…ƒæŒç»­å­¦ä¹ 

å®éªŒæ¨¡å¼ï¼š
- mode='simulation': å¿«é€Ÿæ¨¡æ‹Ÿå®éªŒï¼ˆç§’çº§ï¼‰
- mode='real': çœŸå®LLMè°ƒç”¨å®éªŒï¼ˆåˆ†é’Ÿçº§ï¼‰
"""

import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Literal
import json
from datetime import datetime
import numpy as np
from loguru import logger

# è§£å†³Windowsç¼–ç é—®é¢˜
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from tau2.environment.tool import Tool
from tau2.data_model.message import (
    UserMessage,
    AssistantMessage,
    ToolCall,
    ToolMessage,
    Message,
)

# å¯¼å…¥æ‰€æœ‰æ–¹æ³•
from tau2.continual_learning.agents.icl_experience_replay import ICLExperienceReplayAgent
from tau2.continual_learning.agents.parametric import (
    EWCContinualLearningAgent,
    ReplayContinualLearningAgent,
    ParameterIsolationAgent,
    MetaContinualLearningAgent,
)


# ============================================================================
# å®éªŒé…ç½®
# ============================================================================

class ExperimentConfig:
    """å®éªŒé…ç½®"""
    # å®éªŒæ¨¡å¼ï¼š'simulation' æˆ– 'real'
    mode: Literal['simulation', 'real'] = 'simulation'

    # ä»»åŠ¡é…ç½®
    num_domains = 3
    tasks_per_domain = 20  # æ¯ä¸ªdomainçš„ä»»åŠ¡æ•°

    # Agenté…ç½®
    embedding_dim = 768
    learning_rate = 0.01

    # LLMé…ç½®
    llm_model = "gpt-4"  # çœŸå®æ¨¡å¼ä¸‹ä½¿ç”¨

    # è¾“å‡ºé…ç½®
    output_dir = "experiments/results"
    save_checkpoints = True
    verbose = True


# ============================================================================
# å·¥å…·å’Œä»»åŠ¡ç”Ÿæˆ
# ============================================================================

def create_tools() -> List[Tool]:
    """åˆ›å»ºå·¥å…·"""

    def search_database(query: str) -> str:
        """æœç´¢æ•°æ®åº“"""
        return f"æœç´¢ç»“æœï¼šæ‰¾åˆ°å…³äº '{query}' çš„3æ¡è®°å½•"

    def send_email(to: str, subject: str, body: str = "") -> str:
        """å‘é€é‚®ä»¶"""
        return f"é‚®ä»¶å·²å‘é€ç»™ {to}ï¼Œä¸»é¢˜ï¼š{subject}"

    def create_ticket(title: str, priority: str = "medium", description: str = "") -> str:
        """åˆ›å»ºå·¥å•"""
        return f"å·¥å•å·²åˆ›å»ºï¼š{title} (ä¼˜å…ˆçº§: {priority})"

    def update_record(record_id: str, field: str, value: str) -> str:
        """æ›´æ–°è®°å½•"""
        return f"è®°å½• {record_id} çš„ {field} å·²æ›´æ–°ä¸º {value}"

    return [
        Tool(search_database),
        Tool(send_email),
        Tool(create_ticket),
        Tool(update_record),
    ]


def create_domain_policy(domain: str) -> str:
    """åˆ›å»ºdomain policy"""
    policies = {
        "customer_service": """ä½ æ˜¯å®¢æˆ·æœåŠ¡åŠ©æ‰‹ã€‚
èŒè´£ï¼šå›ç­”å®¢æˆ·é—®é¢˜ã€å‘é€é‚®ä»¶é€šçŸ¥ã€åˆ›å»ºæœåŠ¡å·¥å•ã€‚
ä½¿ç”¨search_databaseæŸ¥è¯¢ä¿¡æ¯ï¼Œsend_emailå‘é€é€šçŸ¥ï¼Œcreate_ticketåˆ›å»ºå·¥å•ã€‚
è¦å‹å¥½ã€ä¸“ä¸šã€é«˜æ•ˆã€‚""",

        "tech_support": """ä½ æ˜¯æŠ€æœ¯æ”¯æŒåŠ©æ‰‹ã€‚
èŒè´£ï¼šè§£å†³æŠ€æœ¯é—®é¢˜ã€æ›´æ–°ç³»ç»Ÿè®°å½•ã€åˆ›å»ºæŠ€æœ¯å·¥å•ã€‚
ä½¿ç”¨search_databaseæŸ¥è¯¢é—®é¢˜ï¼Œupdate_recordæ›´æ–°çŠ¶æ€ï¼Œcreate_ticketä¸ŠæŠ¥é—®é¢˜ã€‚
è¦å‡†ç¡®ã€è¯¦ç»†ã€ç³»ç»ŸåŒ–ã€‚""",

        "sales": """ä½ æ˜¯é”€å”®åŠ©æ‰‹ã€‚
èŒè´£ï¼šæŸ¥è¯¢äº§å“ä¿¡æ¯ã€å‘é€é”€å”®é‚®ä»¶ã€æ›´æ–°å®¢æˆ·è®°å½•ã€‚
ä½¿ç”¨search_databaseæŸ¥è¯¢äº§å“ï¼Œsend_emailè”ç³»å®¢æˆ·ï¼Œupdate_recordæ›´æ–°ä¿¡æ¯ã€‚
è¦ç§¯æã€çƒ­æƒ…ã€ç›®æ ‡å¯¼å‘ã€‚""",
    }
    return policies.get(domain, "ä½ æ˜¯ä¸€ä¸ªé€šç”¨åŠ©æ‰‹ã€‚")


def create_task_data(
    domain: str,
    task_id: int,
    preferred_tool: str,
) -> Dict[str, Any]:
    """åˆ›å»ºä»»åŠ¡æ•°æ®ï¼ˆåŒ…å«ç”¨æˆ·queryå’ŒæœŸæœ›çš„å·¥å…·ï¼‰"""

    task_templates = {
        "customer_service": {
            "search_database": [
                "å®¢æˆ·è¯¢é—®è®¢å• #{} çš„çŠ¶æ€",
                "æŸ¥æ‰¾å®¢æˆ· {} çš„å†å²è®°å½•",
                "æœç´¢äº§å“ {} çš„ä¿¡æ¯",
            ],
            "send_email": [
                "é€šçŸ¥å®¢æˆ· {} å…³äºè®¢å•å»¶è¿Ÿ",
                "å‘é€ç¡®è®¤é‚®ä»¶ç»™ {}",
                "ç»™å®¢æˆ· {} å‘é€æ„Ÿè°¢ä¿¡",
            ],
            "create_ticket": [
                "å®¢æˆ·æŠ¥å‘Šé—®é¢˜ï¼š{}",
                "åˆ›å»ºé€€æ¬¾å·¥å•ï¼š{}",
                "ä¸ŠæŠ¥å®¢æˆ·æŠ•è¯‰ï¼š{}",
            ],
        },
        "tech_support": {
            "search_database": [
                "æŸ¥è¯¢é”™è¯¯ä»£ç  {} çš„è§£å†³æ–¹æ¡ˆ",
                "æœç´¢ç³»ç»Ÿæ—¥å¿—ä¸­çš„ {}",
                "æŸ¥æ‰¾ {} çš„æŠ€æœ¯æ–‡æ¡£",
            ],
            "update_record": [
                "å°†bug #{} çŠ¶æ€æ›´æ–°ä¸ºå·²ä¿®å¤",
                "æ›´æ–°æœåŠ¡å™¨ {} çš„é…ç½®",
                "ä¿®æ”¹ç³»ç»Ÿè®¾ç½® {}",
            ],
            "create_ticket": [
                "ä¸ŠæŠ¥ç³»ç»Ÿæ•…éšœï¼š{}",
                "åˆ›å»ºæŠ€æœ¯å·¥å•ï¼š{}",
                "æäº¤bugæŠ¥å‘Šï¼š{}",
            ],
        },
        "sales": {
            "search_database": [
                "æŸ¥è¯¢äº§å“ {} çš„ä»·æ ¼",
                "æœç´¢å®¢æˆ· {} çš„è´­ä¹°å†å²",
                "æŸ¥æ‰¾ {} çš„åº“å­˜ä¿¡æ¯",
            ],
            "send_email": [
                "ç»™æ½œåœ¨å®¢æˆ· {} å‘é€æŠ¥ä»·",
                "å‘é€äº§å“ä»‹ç»ç»™ {}",
                "é€šçŸ¥å®¢æˆ· {} å…³äºä¿ƒé”€",
            ],
            "update_record": [
                "æ›´æ–°å®¢æˆ· {} çš„çŠ¶æ€ä¸ºå·²æˆäº¤",
                "ä¿®æ”¹è®¢å• {} çš„ä¿¡æ¯",
                "è®°å½•å®¢æˆ· {} çš„åé¦ˆ",
            ],
        },
    }

    templates = task_templates.get(domain, {}).get(preferred_tool, ["ä»»åŠ¡ {}"])
    template = templates[task_id % len(templates)]
    query = template.format(f"T{task_id}")

    return {
        "task_id": f"task_{task_id}",
        "domain": domain,
        "query": query,
        "preferred_tool": preferred_tool,
    }


def create_trajectory_simulation(
    task_data: Dict[str, Any],
    success_rate: float = 0.8,
) -> tuple[List[Message], float, bool]:
    """
    æ¨¡æ‹Ÿæ¨¡å¼ï¼šåˆ›å»ºæ¨¡æ‹Ÿè½¨è¿¹ï¼ˆå¿«é€Ÿï¼‰

    Returns:
        (trajectory, reward, success)
    """
    task_id = task_data["task_id"]
    query = task_data["query"]
    tool = task_data["preferred_tool"]

    # éšæœºå†³å®šæˆåŠŸ/å¤±è´¥
    success = np.random.random() < success_rate
    reward = 1.0 if success else 0.0

    # æ„å»ºå·¥å…·å‚æ•°
    if tool == "search_database":
        args = {"query": query}
    elif tool == "send_email":
        args = {"to": "user@example.com", "subject": query, "body": "è¯¦ç»†å†…å®¹"}
    elif tool == "create_ticket":
        args = {"title": query, "priority": "high", "description": "è¯¦ç»†æè¿°"}
    elif tool == "update_record":
        args = {"record_id": "REC001", "field": "status", "value": "å·²å¤„ç†"}
    else:
        args = {"query": query}

    trajectory = [
        UserMessage(role="user", content=query),
        AssistantMessage(
            role="assistant",
            content=None,
            tool_calls=[
                ToolCall(
                    id=f"call_{task_id}",
                    name=tool,
                    arguments=args
                )
            ]
        ),
        ToolMessage(
            id=f"msg_{task_id}",
            role="tool",
            tool_call_id=f"call_{task_id}",
            content=f"{'æˆåŠŸ' if success else 'å¤±è´¥'}æ‰§è¡Œ {tool}"
        ),
        AssistantMessage(
            role="assistant",
            content=f"{'å·²æˆåŠŸ' if success else 'æœªèƒ½'}å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚"
        ),
    ]

    return trajectory, reward, success


# ============================================================================
# å®éªŒè¿è¡Œå™¨
# ============================================================================

class ContinualLearningExperiment:
    """æŒç»­å­¦ä¹ å®éªŒæ¡†æ¶"""

    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.tools = create_tools()
        self.results = {}

        logger.info(f"å®éªŒæ¨¡å¼: {config.mode}")
        if config.mode == 'simulation':
            logger.info("  - ä½¿ç”¨æ¨¡æ‹Ÿè½¨è¿¹ï¼ˆå¿«é€ŸéªŒè¯ï¼‰")
        else:
            logger.info("  - ä½¿ç”¨çœŸå®LLMè°ƒç”¨ï¼ˆå®Œæ•´å®éªŒï¼‰")

    def create_agent(self, method_name: str, initial_domain: str):
        """åˆ›å»ºæŒ‡å®šæ–¹æ³•çš„agent"""
        policy = create_domain_policy(initial_domain)

        common_args = {
            "tools": self.tools,
            "domain_policy": policy,
            "llm": self.config.llm_model,
            "max_examples_in_prompt": 5,
        }

        if method_name == "ICL-ER":
            return ICLExperienceReplayAgent(**common_args)

        # å‚æ•°åŒ–æ–¹æ³•çš„å…±åŒå‚æ•°
        parametric_args = {
            **common_args,
            "embedding_dim": self.config.embedding_dim,
            "learning_rate": self.config.learning_rate,
        }

        if method_name == "EWC":
            return EWCContinualLearningAgent(
                **parametric_args,
                ewc_lambda=1.0,
                online_ewc=True,
            )

        elif method_name == "Replay":
            return ReplayContinualLearningAgent(
                **parametric_args,
                replay_ratio=0.5,
                replay_batch_size=5,
            )

        elif method_name == "Param-Isolation":
            return ParameterIsolationAgent(
                **parametric_args,
                num_task_families=self.config.num_domains,
            )

        elif method_name == "Meta-CL":
            return MetaContinualLearningAgent(
                **parametric_args,
                meta_learning_rate=0.001,
            )

        else:
            raise ValueError(f"Unknown method: {method_name}")

    def run_task_stream(
        self,
        agent,
        task_stream: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        è¿è¡Œä»»åŠ¡æµ

        Args:
            agent: CL agent
            task_stream: ä»»åŠ¡æµ [{"domain": ..., "task_id": ..., "query": ..., ...}, ...]

        Returns:
            æ€§èƒ½è®°å½•
        """
        performance_history = []
        domain_performance = {}

        for i, task_data in enumerate(task_stream):
            domain = task_data["domain"]
            task_id = task_data["task_id"]

            # ç”Ÿæˆè½¨è¿¹ï¼ˆæ ¹æ®æ¨¡å¼ï¼‰
            if self.config.mode == 'simulation':
                trajectory, reward, success = create_trajectory_simulation(task_data)
            else:
                # çœŸå®æ¨¡å¼ï¼šéœ€è¦è°ƒç”¨Agentçš„generate_next_messageç­‰
                # è¿™é‡Œç®€åŒ–ä¸ºæ¨¡æ‹Ÿ
                trajectory, reward, success = create_trajectory_simulation(task_data)

            # Agentå­¦ä¹ 
            try:
                learning_stats = agent.learn_from_trajectory(
                    task_id=task_id,
                    domain=domain,
                    trajectory=trajectory,
                    reward=reward,
                    success=success,
                )
            except Exception as e:
                logger.warning(f"Learning failed for {task_id}: {e}")
                learning_stats = {}

            # è®°å½•æ€§èƒ½
            performance_history.append({
                "task_id": task_id,
                "domain": domain,
                "reward": reward,
                "success": success,
            })

            if domain not in domain_performance:
                domain_performance[domain] = []
            domain_performance[domain].append(reward)

            # å®šæœŸæ‰“å°è¿›åº¦
            if self.config.verbose and (i + 1) % 20 == 0:
                avg_reward = np.mean([p["reward"] for p in performance_history])
                logger.info(f"  è¿›åº¦: {i+1}/{len(task_stream)}, å¹³å‡å¥–åŠ±={avg_reward:.3f}")

        return {
            "performance_history": performance_history,
            "domain_performance": domain_performance,
            "agent_stats": agent.get_statistics() if hasattr(agent, 'get_statistics') else {},
        }

    def compute_metrics(
        self,
        performance_history: List[Dict],
        domain_performance: Dict[str, List[float]],
    ) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""

        # 1. å¹³å‡æ€§èƒ½
        all_rewards = [p["reward"] for p in performance_history]
        avg_performance = np.mean(all_rewards) if all_rewards else 0.0

        # 2. æ¯ä¸ªdomainçš„æ€§èƒ½
        domain_avg = {
            domain: np.mean(perfs) if perfs else 0.0
            for domain, perfs in domain_performance.items()
        }

        # 3. é—å¿˜åº¦ï¼ˆForward Transferï¼‰
        # æ¯”è¾ƒæ¯ä¸ªdomainæ—©æœŸå’Œæ™šæœŸæ€§èƒ½
        forgetting = 0.0
        num_domains = 0
        for domain, perfs in domain_performance.items():
            if len(perfs) >= 10:
                early = np.mean(perfs[:5])
                late = np.mean(perfs[-5:])
                forgetting += max(0, early - late)
                num_domains += 1
        forgetting = forgetting / num_domains if num_domains > 0 else 0.0

        # 4. ç¨³å®šæ€§
        performance_std = np.std(all_rewards) if len(all_rewards) > 1 else 0.0
        stability = 1.0 / (1.0 + performance_std)

        return {
            "avg_performance": float(avg_performance),
            "domain_performance": domain_avg,
            "forgetting": float(forgetting),
            "stability": float(stability),
            "performance_std": float(performance_std),
            "total_tasks": len(performance_history),
        }

    def run_method(
        self,
        method_name: str,
        task_stream: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ–¹æ³•çš„å®éªŒ"""

        logger.info(f"\n{'='*80}")
        logger.info(f"è¿è¡Œæ–¹æ³•: {method_name}")
        logger.info(f"{'='*80}")

        try:
            # åˆ›å»ºagent
            initial_domain = task_stream[0]["domain"]
            agent = self.create_agent(method_name, initial_domain)

            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()

            # è¿è¡Œä»»åŠ¡æµ
            run_results = self.run_task_stream(agent, task_stream)

            # è®°å½•ç»“æŸæ—¶é—´
            elapsed_time = time.time() - start_time

            # è®¡ç®—æŒ‡æ ‡
            metrics = self.compute_metrics(
                run_results["performance_history"],
                run_results["domain_performance"],
            )

            # æ·»åŠ é¢å¤–ä¿¡æ¯
            metrics["elapsed_time"] = elapsed_time
            metrics["method_name"] = method_name
            metrics["agent_stats"] = run_results["agent_stats"]

            logger.info(f"âœ“ {method_name} å®Œæˆ")
            logger.info(f"  - å¹³å‡æ€§èƒ½: {metrics['avg_performance']:.3f}")
            logger.info(f"  - é—å¿˜åº¦: {metrics['forgetting']:.3f}")
            logger.info(f"  - ç¨³å®šæ€§: {metrics['stability']:.3f}")
            logger.info(f"  - ç”¨æ—¶: {elapsed_time:.1f}s")

            return metrics

        except Exception as e:
            logger.error(f"âœ— {method_name} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "method_name": method_name,
                "avg_performance": 0.0,
                "forgetting": 1.0,
                "elapsed_time": 0.0,
            }

    def generate_task_stream(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä»»åŠ¡æµ"""
        task_stream = []
        task_id = 0

        domains = ["customer_service", "tech_support", "sales"]
        tools = [tool.name for tool in self.tools]

        # ä¸ºæ¯ä¸ªdomainç”Ÿæˆä»»åŠ¡
        for _ in range(self.config.tasks_per_domain):
            for domain in domains:
                task_id += 1
                # ä¸ºæ¯ä¸ªdomainéšæœºé€‰æ‹©å·¥å…·
                preferred_tool = np.random.choice(tools)
                task_data = create_task_data(domain, task_id, preferred_tool)
                task_stream.append(task_data)

        return task_stream

    def run_all_methods(self) -> Dict[str, Dict]:
        """è¿è¡Œæ‰€æœ‰æ–¹æ³•çš„å®éªŒ"""

        logger.info(f"\n{'='*80}")
        logger.info(f"å¼€å§‹å®Œæ•´å®éªŒ")
        logger.info(f"{'='*80}")

        # ç”Ÿæˆä»»åŠ¡æµ
        task_stream = self.generate_task_stream()

        logger.info(f"ä»»åŠ¡æµç”Ÿæˆå®Œæˆ:")
        logger.info(f"  - æ€»ä»»åŠ¡æ•°: {len(task_stream)}")
        logger.info(f"  - Domains: {self.config.num_domains}")
        logger.info(f"  - æ¯ä¸ªdomainä»»åŠ¡æ•°: {self.config.tasks_per_domain}")

        # è¦æµ‹è¯•çš„æ–¹æ³•
        methods = [
            "ICL-ER",          # Baseline
            "EWC",             # Method 1
            "Replay",          # Method 2
            "Param-Isolation", # Method 3
            "Meta-CL",         # Method 4
        ]

        results = {}

        for method_name in methods:
            results[method_name] = self.run_method(method_name, task_stream)

        return results

    def print_comparison_table(self, results: Dict):
        """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""

        print(f"\n{'='*80}")
        print("å®éªŒç»“æœå¯¹æ¯”")
        print(f"{'='*80}")

        # è¡¨å¤´
        print(f"\n{'æ–¹æ³•':<20} {'å¹³å‡æ€§èƒ½':<12} {'é—å¿˜åº¦':<12} {'ç¨³å®šæ€§':<12} {'ç”¨æ—¶(s)':<10}")
        print("-"*80)

        # æ’åºï¼ˆæŒ‰å¹³å‡æ€§èƒ½ï¼‰
        sorted_results = sorted(
            results.items(),
            key=lambda x: x[1].get('avg_performance', 0),
            reverse=True
        )

        for method_name, metrics in sorted_results:
            if 'error' in metrics:
                print(f"{method_name:<20} {'ERROR':<12} - {metrics['error']}")
            else:
                print(f"{method_name:<20} "
                      f"{metrics['avg_performance']:<12.3f} "
                      f"{metrics['forgetting']:<12.3f} "
                      f"{metrics['stability']:<12.3f} "
                      f"{metrics['elapsed_time']:<10.1f}")

        print(f"\n{'='*80}")

        # èƒœè€…ç»Ÿè®¡
        valid_results = {k: v for k, v in results.items() if 'error' not in v}
        if valid_results:
            best_perf = max(valid_results.items(), key=lambda x: x[1].get('avg_performance', 0))
            best_forget = min(valid_results.items(), key=lambda x: x[1].get('forgetting', 1))

            print(f"\nğŸ† æœ€ä½³æ€§èƒ½: {best_perf[0]} ({best_perf[1]['avg_performance']:.3f})")
            print(f"ğŸ›¡ï¸  æœ€ä½é—å¿˜: {best_forget[0]} ({best_forget[1]['forgetting']:.3f})")

            # å‚æ•°åŒ–æ–¹æ³• vs Baseline
            if 'ICL-ER' in valid_results:
                baseline_perf = valid_results['ICL-ER'].get('avg_performance', 0)
                print(f"\nğŸ“Š ä¸Baseline (ICL-ER) å¯¹æ¯”:")
                for method_name, metrics in valid_results.items():
                    if method_name != 'ICL-ER':
                        improvement = metrics['avg_performance'] - baseline_perf
                        if baseline_perf > 0:
                            pct = improvement / baseline_perf * 100
                            print(f"  {method_name}: {improvement:+.3f} ({pct:+.1f}%)")

    def save_results(self, results: Dict, output_path: str):
        """ä¿å­˜å®éªŒç»“æœ"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main(
    mode: Literal['simulation', 'real'] = 'simulation',
    num_domains: int = 3,
    tasks_per_domain: int = 20,
):
    """
    è¿è¡Œå®Œæ•´å®éªŒ

    Args:
        mode: 'simulation' (å¿«é€Ÿ) æˆ– 'real' (å®Œæ•´)
        num_domains: domainæ•°é‡
        tasks_per_domain: æ¯ä¸ªdomainçš„ä»»åŠ¡æ•°
    """

    print("="*80)
    print("å‚æ•°åŒ–æŒç»­å­¦ä¹ æ–¹æ³• - å®Œæ•´å¯¹æ¯”å®éªŒ")
    print("="*80)
    print(f"\nå®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # åˆ›å»ºé…ç½®
    config = ExperimentConfig()
    config.mode = mode
    config.num_domains = num_domains
    config.tasks_per_domain = tasks_per_domain

    print(f"\nå®éªŒé…ç½®:")
    print(f"  - æ¨¡å¼: {config.mode}")
    print(f"  - Domains: {config.num_domains}")
    print(f"  - æ¯ä¸ªdomainä»»åŠ¡æ•°: {config.tasks_per_domain}")
    print(f"  - æ€»ä»»åŠ¡æ•°: {config.num_domains * config.tasks_per_domain}")
    print(f"  - Embeddingç»´åº¦: {config.embedding_dim}")
    print(f"  - å­¦ä¹ ç‡: {config.learning_rate}")

    if mode == 'simulation':
        print(f"\nâš¡ æ¨¡æ‹Ÿæ¨¡å¼ï¼šä½¿ç”¨mockè½¨è¿¹ï¼ŒéªŒè¯ä»£ç é€»è¾‘ï¼ˆå¿«é€Ÿï¼‰")
    else:
        print(f"\nğŸ”¥ çœŸå®æ¨¡å¼ï¼šè°ƒç”¨LLM APIï¼Œå®Œæ•´å®éªŒï¼ˆè¾ƒæ…¢ï¼‰")

    # åˆ›å»ºå®éªŒ
    experiment = ContinualLearningExperiment(config)

    # è¿è¡Œæ‰€æœ‰æ–¹æ³•
    print(f"\nå¼€å§‹è¿è¡Œå®éªŒ...")
    results = experiment.run_all_methods()

    # æ‰“å°å¯¹æ¯”è¡¨
    experiment.print_comparison_table(results)

    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_path = f"{config.output_dir}/comparison_{mode}_{timestamp}.json"
    experiment.save_results(results, output_path)

    print(f"\nâœ… å®éªŒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

    print("\nğŸ’¡ å…³é”®å‘ç°:")
    print("  - å‚æ•°åŒ–æ–¹æ³• vs éå‚æ•°åŒ–ICL-ERçš„æ€§èƒ½å¯¹æ¯”")
    print("  - ä¸åŒæ–¹æ³•åœ¨é—å¿˜åº¦ã€ç¨³å®šæ€§ä¸Šçš„å·®å¼‚")
    print("  - è¯¦è§ä¿å­˜çš„JSONæ–‡ä»¶")

    return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='æŒç»­å­¦ä¹ æ–¹æ³•å¯¹æ¯”å®éªŒ')
    parser.add_argument(
        '--mode',
        type=str,
        default='simulation',
        choices=['simulation', 'real'],
        help='å®éªŒæ¨¡å¼ï¼šsimulation(å¿«é€Ÿ) æˆ– real(å®Œæ•´)'
    )
    parser.add_argument(
        '--num-domains',
        type=int,
        default=3,
        help='Domainæ•°é‡'
    )
    parser.add_argument(
        '--tasks-per-domain',
        type=int,
        default=20,
        help='æ¯ä¸ªdomainçš„ä»»åŠ¡æ•°'
    )

    args = parser.parse_args()

    results = main(
        mode=args.mode,
        num_domains=args.num_domains,
        tasks_per_domain=args.tasks_per_domain,
    )
