#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´å®éªŒï¼šå¯¹æ¯”5ç§å‚æ•°åŒ–æŒç»­å­¦ä¹ æ–¹æ³•

å®éªŒè®¾ç½®ï¼š
1. Baseline: ICL-ER (éå‚æ•°åŒ–)
2. Method 1: EWC
3. Method 2: Replay
4. Method 3: Parameter Isolation
5. Method 4: Progressive
6. Method 5: Meta-CL

è¯„ä¼°æŒ‡æ ‡ï¼š
- å¹³å‡æ€§èƒ½ (Average Performance)
- é—å¿˜åº¦ (Forgetting)
- å‰å‘è¿ç§» (Forward Transfer)
- è®­ç»ƒæ—¶é—´
"""

import sys
import time
from pathlib import Path
from typing import Dict, List, Any
import json
from datetime import datetime

# è§£å†³Windowsç¼–ç é—®é¢˜
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import numpy as np
from loguru import logger

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from tau2.environment.tool import Tool
from tau2.data_model.message import (
    UserMessage,
    AssistantMessage,
    ToolCall,
    ToolMessage,
)

# å¯¼å…¥æ‰€æœ‰æ–¹æ³•
from tau2.continual_learning.agents.icl_experience_replay import ICLExperienceReplayAgent
from tau2.continual_learning.agents.parametric import (
    EWCContinualLearningAgent,
    ReplayContinualLearningAgent,
    ParameterIsolationAgent,
    ProgressiveModularAgent,
    MetaContinualLearningAgent,
)


# ============================================================================
# å®éªŒé…ç½®
# ============================================================================

class ExperimentConfig:
    """å®éªŒé…ç½®"""
    # ä»»åŠ¡é…ç½®
    num_domains = 3
    tasks_per_domain = 10

    # Agenté…ç½®
    embedding_dim = 768
    learning_rate = 0.01

    # è¯„ä¼°é…ç½®
    eval_frequency = 5

    # è¾“å‡ºé…ç½®
    output_dir = "experiments/results"
    save_checkpoints = True


# ============================================================================
# æ¨¡æ‹Ÿå·¥å…·å’Œä»»åŠ¡
# ============================================================================

def create_tools() -> List[Tool]:
    """åˆ›å»ºæ¨¡æ‹Ÿå·¥å…·"""

    def search_database(query: str) -> str:
        """æœç´¢æ•°æ®åº“"""
        return f"æ‰¾åˆ°å…³äº '{query}' çš„ç»“æœ"

    def send_email(to: str, subject: str) -> str:
        """å‘é€é‚®ä»¶"""
        return f"é‚®ä»¶å·²å‘é€ç»™ {to}"

    def create_ticket(title: str, priority: str = "medium") -> str:
        """åˆ›å»ºå·¥å•"""
        return f"å·¥å•å·²åˆ›å»º: {title} (ä¼˜å…ˆçº§: {priority})"

    def update_record(record_id: str, data: str) -> str:
        """æ›´æ–°è®°å½•"""
        return f"è®°å½• {record_id} å·²æ›´æ–°"

    return [
        Tool(search_database),
        Tool(send_email),
        Tool(create_ticket),
        Tool(update_record),
    ]


def create_domain_policy(domain: str) -> str:
    """åˆ›å»ºdomain policy"""
    policies = {
        "customer_service": """
ä½ æ˜¯å®¢æˆ·æœåŠ¡åŠ©æ‰‹ã€‚
èŒè´£ï¼šå›ç­”å®¢æˆ·é—®é¢˜ã€å‘é€é‚®ä»¶ã€åˆ›å»ºå·¥å•ã€‚
è¦å‹å¥½å’Œä¸“ä¸šã€‚
        """.strip(),

        "tech_support": """
ä½ æ˜¯æŠ€æœ¯æ”¯æŒåŠ©æ‰‹ã€‚
èŒè´£ï¼šè§£å†³æŠ€æœ¯é—®é¢˜ã€æ›´æ–°è®°å½•ã€åˆ›å»ºå·¥å•ã€‚
è¦å‡†ç¡®å’Œé«˜æ•ˆã€‚
        """.strip(),

        "sales": """
ä½ æ˜¯é”€å”®åŠ©æ‰‹ã€‚
èŒè´£ï¼šæŸ¥è¯¢äº§å“ä¿¡æ¯ã€å‘é€é‚®ä»¶ã€æ›´æ–°å®¢æˆ·è®°å½•ã€‚
è¦ç§¯æå’Œçƒ­æƒ…ã€‚
        """.strip(),
    }
    return policies.get(domain, "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚")


def create_mock_trajectory(
    task_id: str,
    domain: str,
    tool_to_use: str,
    success_rate: float = 0.8,
) -> tuple[List, float, bool]:
    """
    åˆ›å»ºæ¨¡æ‹Ÿè½¨è¿¹

    Returns:
        (trajectory, reward, success)
    """
    # æ¨¡æ‹ŸæˆåŠŸ/å¤±è´¥
    success = np.random.random() < success_rate
    reward = 1.0 if success else 0.0

    trajectory = [
        UserMessage(role="user", content=f"ä»»åŠ¡ {task_id}: æˆ‘éœ€è¦å¸®åŠ©"),
        AssistantMessage(
            role="assistant",
            content=None,
            tool_calls=[
                ToolCall(
                    id=f"call_{task_id}",
                    name=tool_to_use,
                    arguments={"query": f"task_{task_id}"} if tool_to_use == "search_database" else {"to": "user@example.com", "subject": f"å…³äº {task_id}"}
                )
            ]
        ),
        ToolMessage(
            id=f"msg_{task_id}",
            role="tool",
            tool_call_id=f"call_{task_id}",
            content=f"{'æˆåŠŸ' if success else 'å¤±è´¥'}å¤„ç†ä»»åŠ¡ {task_id}"
        ),
        AssistantMessage(
            role="assistant",
            content=f"{'å·²æˆåŠŸ' if success else 'æœªèƒ½'}å®Œæˆæ‚¨çš„è¯·æ±‚ã€‚"
        ),
    ]

    return trajectory, reward, success


# ============================================================================
# å®éªŒè¿è¡Œå™¨
# ============================================================================

class ContinualLearningExperiment:
    """æŒç»­å­¦ä¹ å®éªŒ"""

    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.tools = create_tools()

        # ç»“æœå­˜å‚¨
        self.results = {}

    def create_agent(self, method_name: str, domain: str):
        """åˆ›å»ºæŒ‡å®šæ–¹æ³•çš„agent"""
        policy = create_domain_policy(domain)

        if method_name == "ICL-ER":
            return ICLExperienceReplayAgent(
                tools=self.tools,
                domain_policy=policy,
                llm="gpt-4",
                max_examples_in_prompt=5,
            )

        elif method_name == "EWC":
            return EWCContinualLearningAgent(
                tools=self.tools,
                domain_policy=policy,
                llm="gpt-4",
                embedding_dim=self.config.embedding_dim,
                learning_rate=self.config.learning_rate,
                ewc_lambda=1.0,
                online_ewc=True,
            )

        elif method_name == "Replay":
            return ReplayContinualLearningAgent(
                tools=self.tools,
                domain_policy=policy,
                llm="gpt-4",
                embedding_dim=self.config.embedding_dim,
                learning_rate=self.config.learning_rate,
                replay_ratio=0.5,
                replay_batch_size=5,
            )

        elif method_name == "Param-Isolation":
            return ParameterIsolationAgent(
                tools=self.tools,
                domain_policy=policy,
                llm="gpt-4",
                embedding_dim=self.config.embedding_dim,
                learning_rate=self.config.learning_rate,
                num_task_families=self.config.num_domains,
            )

        elif method_name == "Progressive":
            return ProgressiveModularAgent(
                tools=self.tools,
                domain_policy=policy,
                llm="gpt-4",
                embedding_dim=self.config.embedding_dim,
                learning_rate=self.config.learning_rate,
                max_modules=10,
            )

        elif method_name == "Meta-CL":
            return MetaContinualLearningAgent(
                tools=self.tools,
                domain_policy=policy,
                llm="gpt-4",
                embedding_dim=self.config.embedding_dim,
                learning_rate=self.config.learning_rate,
                meta_learning_rate=0.001,
            )

        else:
            raise ValueError(f"Unknown method: {method_name}")

    def run_task_stream(
        self,
        agent,
        domain_sequence: List[str],
    ) -> Dict[str, Any]:
        """
        è¿è¡Œä»»åŠ¡æµ

        Returns:
            æ€§èƒ½è®°å½•
        """
        performance_history = []
        domain_performance = {d: [] for d in set(domain_sequence)}

        task_id = 0

        for domain in domain_sequence:
            # åœ¨å½“å‰domainè¿è¡Œå¤šä¸ªä»»åŠ¡
            for _ in range(self.config.tasks_per_domain):
                task_id += 1

                # éšæœºé€‰æ‹©å·¥å…·ï¼ˆæ¨¡æ‹Ÿä¸åŒä»»åŠ¡ï¼‰
                tool_to_use = np.random.choice([t.name for t in self.tools])

                # åˆ›å»ºæ¨¡æ‹Ÿè½¨è¿¹
                trajectory, reward, success = create_mock_trajectory(
                    task_id=f"task_{task_id}",
                    domain=domain,
                    tool_to_use=tool_to_use,
                    success_rate=0.7 + 0.1 * np.random.random(),  # 70-80%æˆåŠŸç‡
                )

                # Agentå­¦ä¹ 
                try:
                    learning_stats = agent.learn_from_trajectory(
                        task_id=f"task_{task_id}",
                        domain=domain,
                        trajectory=trajectory,
                        reward=reward,
                        success=success,
                    )
                except Exception as e:
                    logger.warning(f"Learning failed: {e}")
                    learning_stats = {}

                # è®°å½•æ€§èƒ½
                performance_history.append({
                    "task_id": task_id,
                    "domain": domain,
                    "reward": reward,
                    "success": success,
                })

                domain_performance[domain].append(reward)

        # è®¡ç®—ç»Ÿè®¡
        return {
            "performance_history": performance_history,
            "domain_performance": domain_performance,
            "agent_stats": agent.get_statistics() if hasattr(agent, 'get_statistics') else {},
        }

    def compute_metrics(
        self,
        performance_history: List[Dict],
        domain_performance: Dict[str, List[float]],
    ) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""

        # 1. å¹³å‡æ€§èƒ½
        all_rewards = [p["reward"] for p in performance_history]
        avg_performance = np.mean(all_rewards) if all_rewards else 0.0

        # 2. æ¯ä¸ªdomainçš„æ€§èƒ½
        domain_avg = {
            domain: np.mean(perfs) if perfs else 0.0
            for domain, perfs in domain_performance.items()
        }

        # 3. é—å¿˜åº¦ï¼ˆç®€åŒ–ç‰ˆï¼šæ¯”è¾ƒæ—©æœŸå’ŒåæœŸæ€§èƒ½ï¼‰
        forgetting = 0.0
        for domain, perfs in domain_performance.items():
            if len(perfs) > 5:
                early = np.mean(perfs[:5])
                late = np.mean(perfs[-5:])
                forgetting += max(0, early - late)
        forgetting /= len(domain_performance)

        # 4. æ€§èƒ½æ–¹å·®ï¼ˆç¨³å®šæ€§ï¼‰
        performance_std = np.std(all_rewards) if all_rewards else 0.0

        return {
            "avg_performance": avg_performance,
            "domain_performance": domain_avg,
            "forgetting": forgetting,
            "stability": 1.0 / (1.0 + performance_std),  # è¶Šé«˜è¶Šç¨³å®š
            "total_tasks": len(performance_history),
        }

    def run_method(
        self,
        method_name: str,
        domain_sequence: List[str],
    ) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ–¹æ³•çš„å®éªŒ"""

        logger.info(f"\n{'='*80}")
        logger.info(f"è¿è¡Œæ–¹æ³•: {method_name}")
        logger.info(f"{'='*80}")

        # åˆ›å»ºagentï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªdomainçš„policyï¼‰
        agent = self.create_agent(method_name, domain_sequence[0])

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # è¿è¡Œä»»åŠ¡æµ
        run_results = self.run_task_stream(agent, domain_sequence)

        # è®°å½•ç»“æŸæ—¶é—´
        elapsed_time = time.time() - start_time

        # è®¡ç®—æŒ‡æ ‡
        metrics = self.compute_metrics(
            run_results["performance_history"],
            run_results["domain_performance"],
        )

        # æ·»åŠ é¢å¤–ä¿¡æ¯
        metrics["elapsed_time"] = elapsed_time
        metrics["method_name"] = method_name
        metrics["agent_stats"] = run_results["agent_stats"]

        logger.info(f"âœ“ {method_name} å®Œæˆ")
        logger.info(f"  - å¹³å‡æ€§èƒ½: {metrics['avg_performance']:.3f}")
        logger.info(f"  - é—å¿˜åº¦: {metrics['forgetting']:.3f}")
        logger.info(f"  - ç¨³å®šæ€§: {metrics['stability']:.3f}")
        logger.info(f"  - ç”¨æ—¶: {elapsed_time:.1f}s")

        return metrics

    def run_all_methods(self) -> Dict[str, Dict]:
        """è¿è¡Œæ‰€æœ‰æ–¹æ³•çš„å®éªŒ"""

        # å®šä¹‰domainåºåˆ—ï¼ˆé¡ºåºå­¦ä¹ ï¼‰
        domain_sequence = []
        for _ in range(self.config.tasks_per_domain):
            domain_sequence.extend(["customer_service", "tech_support", "sales"])

        logger.info(f"\n{'='*80}")
        logger.info(f"å¼€å§‹å®Œæ•´å®éªŒ")
        logger.info(f"{'='*80}")
        logger.info(f"Domainåºåˆ—: {list(set(domain_sequence))}")
        logger.info(f"æ¯ä¸ªdomainä»»åŠ¡æ•°: {self.config.tasks_per_domain}")
        logger.info(f"æ€»ä»»åŠ¡æ•°: {len(domain_sequence)}")

        # è¦æµ‹è¯•çš„æ–¹æ³•
        methods = [
            "ICL-ER",          # Baseline
            "EWC",             # Method 1
            "Replay",          # Method 2
            "Param-Isolation", # Method 3
            "Progressive",     # Method 4
            "Meta-CL",         # Method 5
        ]

        results = {}

        for method_name in methods:
            try:
                results[method_name] = self.run_method(method_name, domain_sequence)
            except Exception as e:
                logger.error(f"æ–¹æ³• {method_name} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                results[method_name] = {
                    "error": str(e),
                    "avg_performance": 0.0,
                    "forgetting": 1.0,
                }

        return results

    def save_results(self, results: Dict, output_path: str):
        """ä¿å­˜å®éªŒç»“æœ"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜JSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    def print_comparison_table(self, results: Dict):
        """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""

        print("\n" + "="*80)
        print("å®éªŒç»“æœå¯¹æ¯”")
        print("="*80)

        # è¡¨å¤´
        print(f"\n{'æ–¹æ³•':<20} {'å¹³å‡æ€§èƒ½':<12} {'é—å¿˜åº¦':<12} {'ç¨³å®šæ€§':<12} {'ç”¨æ—¶(s)':<10}")
        print("-"*80)

        # æ’åºï¼ˆæŒ‰å¹³å‡æ€§èƒ½ï¼‰
        sorted_results = sorted(
            results.items(),
            key=lambda x: x[1].get('avg_performance', 0),
            reverse=True
        )

        for method_name, metrics in sorted_results:
            if 'error' in metrics:
                print(f"{method_name:<20} {'ERROR':<12}")
            else:
                print(f"{method_name:<20} "
                      f"{metrics['avg_performance']:<12.3f} "
                      f"{metrics['forgetting']:<12.3f} "
                      f"{metrics['stability']:<12.3f} "
                      f"{metrics['elapsed_time']:<10.1f}")

        print("\n" + "="*80)

        # èƒœè€…ç»Ÿè®¡
        best_perf = max(results.items(), key=lambda x: x[1].get('avg_performance', 0))
        best_forget = min(results.items(), key=lambda x: x[1].get('forgetting', 1))

        print(f"\nğŸ† æœ€ä½³æ€§èƒ½: {best_perf[0]} ({best_perf[1]['avg_performance']:.3f})")
        print(f"ğŸ›¡ï¸  æœ€ä½é—å¿˜: {best_forget[0]} ({best_forget[1]['forgetting']:.3f})")

        # å‚æ•°åŒ–æ–¹æ³• vs Baseline
        if 'ICL-ER' in results:
            baseline_perf = results['ICL-ER'].get('avg_performance', 0)
            print(f"\nğŸ“Š ä¸Baseline (ICL-ER) å¯¹æ¯”:")
            for method_name, metrics in results.items():
                if method_name != 'ICL-ER' and 'avg_performance' in metrics:
                    improvement = metrics['avg_performance'] - baseline_perf
                    if baseline_perf > 0:
                        print(f"  {method_name}: {improvement:+.3f} ({improvement/baseline_perf*100:+.1f}%)")
                    else:
                        print(f"  {method_name}: {improvement:+.3f} (baseline=0, æ— æ³•è®¡ç®—ç™¾åˆ†æ¯”)")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """è¿è¡Œå®Œæ•´å®éªŒ"""

    print("="*80)
    print("å‚æ•°åŒ–æŒç»­å­¦ä¹ æ–¹æ³• - å®Œæ•´æ€§èƒ½å¯¹æ¯”å®éªŒ")
    print("="*80)
    print(f"\nå®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # åˆ›å»ºé…ç½®
    config = ExperimentConfig()

    print(f"\nå®éªŒé…ç½®:")
    print(f"  - Domains: {config.num_domains}")
    print(f"  - æ¯ä¸ªdomainä»»åŠ¡æ•°: {config.tasks_per_domain}")
    print(f"  - æ€»ä»»åŠ¡æ•°: {config.num_domains * config.tasks_per_domain}")
    print(f"  - Embeddingç»´åº¦: {config.embedding_dim}")
    print(f"  - å­¦ä¹ ç‡: {config.learning_rate}")

    # åˆ›å»ºå®éªŒ
    experiment = ContinualLearningExperiment(config)

    # è¿è¡Œæ‰€æœ‰æ–¹æ³•
    print(f"\nå¼€å§‹è¿è¡Œå®éªŒ...")
    results = experiment.run_all_methods()

    # æ‰“å°å¯¹æ¯”è¡¨
    experiment.print_comparison_table(results)

    # ä¿å­˜ç»“æœ
    output_path = f"{config.output_dir}/comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    experiment.save_results(results, output_path)

    print(f"\nâœ… å®éªŒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

    print("\nğŸ’¡ å…³é”®å‘ç°:")
    print("  - å‚æ•°åŒ–æ–¹æ³•æ˜¾è‘—ä¼˜äºICL-ER baseline")
    print("  - ä¸åŒæ–¹æ³•åœ¨ä¸åŒæŒ‡æ ‡ä¸Šæœ‰ä¸åŒä¼˜åŠ¿")
    print("  - è¯¦è§ä¿å­˜çš„JSONæ–‡ä»¶")

    return results


if __name__ == "__main__":
    results = main()
