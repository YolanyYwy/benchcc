#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–å®éªŒï¼šå¯¹æ¯”å‚æ•°åŒ–ç»„ä»¶æ€§èƒ½

ç›´æ¥æµ‹è¯•æ ¸å¿ƒç»„ä»¶ï¼ˆTool Scorerå’ŒParametric Memoryï¼‰ï¼Œ
ä¸ä¾èµ–å®Œæ•´çš„Agentå®ä¾‹åŒ–ã€‚
"""

import sys
import time
from pathlib import Path
from typing import Dict, List, Any
import json
from datetime import datetime

# è§£å†³Windowsç¼–ç é—®é¢˜
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import numpy as np
from loguru import logger

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from tau2.environment.tool import Tool
from tau2.continual_learning.agents.parametric.tool_scorer import ToolScorer
from tau2.continual_learning.agents.parametric.parametric_memory import ParametricMemory
from tau2.continual_learning.memory.buffer import Experience


# ============================================================================
# å®éªŒé…ç½®
# ============================================================================

class Config:
    """å®éªŒé…ç½®"""
    num_domains = 3
    tasks_per_domain = 20
    embedding_dim = 768
    learning_rate = 0.01
    num_tools = 4


# ============================================================================
# æ ¸å¿ƒç»„ä»¶æ€§èƒ½æµ‹è¯•
# ============================================================================

def create_tools():
    """åˆ›å»ºæ¨¡æ‹Ÿå·¥å…·"""
    def search(query: str) -> str:
        return f"æœç´¢: {query}"

    def email(to: str, subject: str) -> str:
        return f"é‚®ä»¶ç»™: {to}"

    def ticket(title: str) -> str:
        return f"å·¥å•: {title}"

    def update(record_id: str) -> str:
        return f"æ›´æ–°: {record_id}"

    return [Tool(search), Tool(email), Tool(ticket), Tool(update)]


def test_tool_scorer_learning(config: Config) -> Dict[str, Any]:
    """æµ‹è¯•Tool Scorerå­¦ä¹ èƒ½åŠ›"""

    logger.info("\n" + "="*80)
    logger.info("æµ‹è¯•1: Tool Scorerå­¦ä¹ èƒ½åŠ›")
    logger.info("="*80)

    tools = create_tools()
    scorer = ToolScorer(
        tools=tools,
        embedding_dim=config.embedding_dim,
        learning_rate=config.learning_rate,
    )

    # è®°å½•æ€§èƒ½
    performance = []
    param_changes = []

    initial_weights = scorer.weights.copy()

    # æ¨¡æ‹Ÿå­¦ä¹ è¿‡ç¨‹
    num_tasks = config.num_domains * config.tasks_per_domain
    correct_count = 0

    for task_id in range(num_tasks):
        # æ¨¡æ‹ŸçŠ¶æ€
        state_emb = np.random.randn(config.embedding_dim)

        # æ¨¡æ‹Ÿæ­£ç¡®çš„å·¥å…·ï¼ˆdomainç‰¹å®šï¼‰
        domain_id = task_id // config.tasks_per_domain
        correct_tool = tools[domain_id % len(tools)].name

        # è·å–å½“å‰é¢„æµ‹
        probs = scorer.get_tool_probabilities(state_emb)
        predicted_tool = max(probs.items(), key=lambda x: x[1])[0]

        # æ£€æŸ¥æ˜¯å¦æ­£ç¡®
        is_correct = (predicted_tool == correct_tool)
        if is_correct:
            correct_count += 1

        # æ¨¡æ‹Ÿå¥–åŠ±
        reward = 1.0 if is_correct else 0.0

        # æ›´æ–°å‚æ•°
        scorer.update_weights(
            state_embedding=state_emb,
            selected_tool=correct_tool,
            reward=reward,
            success=is_correct,
        )

        # è®°å½•æ€§èƒ½
        accuracy = correct_count / (task_id + 1)
        performance.append(accuracy)

        # è®°å½•å‚æ•°å˜åŒ–
        param_change = np.linalg.norm(scorer.weights - initial_weights)
        param_changes.append(param_change)

        if (task_id + 1) % 10 == 0:
            logger.info(f"  Task {task_id+1:3d}: å‡†ç¡®ç‡={accuracy:.3f}, å‚æ•°å˜åŒ–={param_change:.4f}")

    # è®¡ç®—Fisher Information
    state_embeddings = [np.random.randn(config.embedding_dim) for _ in range(50)]
    selected_tools = [tools[i % len(tools)].name for i in range(50)]
    scorer.compute_fisher_information(state_embeddings, selected_tools)

    final_accuracy = performance[-1]
    total_param_change = param_changes[-1]

    logger.info(f"\nç»“æœ:")
    logger.info(f"  - æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.3f}")
    logger.info(f"  - æ€»å‚æ•°å˜åŒ–: {total_param_change:.4f}")
    logger.info(f"  - Fisherå‡å€¼: {scorer.fisher_information.mean():.6f}")

    return {
        "method": "ToolScorer",
        "final_accuracy": final_accuracy,
        "param_change": total_param_change,
        "performance_history": performance,
        "fisher_mean": float(scorer.fisher_information.mean()),
    }


def test_parametric_memory_learning(config: Config) -> Dict[str, Any]:
    """æµ‹è¯•Parametric Memoryå­¦ä¹ èƒ½åŠ›"""

    logger.info("\n" + "="*80)
    logger.info("æµ‹è¯•2: Parametric Memoryå­¦ä¹ èƒ½åŠ›")
    logger.info("="*80)

    memory = ParametricMemory(
        max_size=100,
        embedding_dim=config.embedding_dim,
        learning_rate=config.learning_rate,
        initial_importance=1.0,
    )

    # æ·»åŠ ç»éªŒå¹¶å­¦ä¹ é‡è¦æ€§
    num_experiences = config.num_domains * config.tasks_per_domain

    importance_changes = []

    for i in range(num_experiences):
        # åˆ›å»ºç»éªŒ
        exp = Experience(
            experience_id=f"exp_{i}",
            task_id=f"task_{i}",
            domain=f"domain_{i % config.num_domains}",
            timestamp=datetime.now(),
            observation=f"è§‚å¯Ÿ {i}",
            action=f"åŠ¨ä½œ {i}",
            reward=0.5 + 0.5 * np.random.random(),
            success=np.random.random() > 0.3,
            embedding=list(np.random.randn(config.embedding_dim)),
        )

        memory.add(exp)

        # æ¨¡æ‹Ÿä½¿ç”¨å’Œé‡è¦æ€§æ›´æ–°
        if i % 5 == 0 and i > 0:
            # é‡‡æ ·ä¸€äº›ç»éªŒ
            sampled = memory.sample_by_importance(n=5)

            # åŸºäº"æ•ˆç”¨"æ›´æ–°é‡è¦æ€§
            for exp in sampled:
                utility = np.random.random()
                gradient = utility - 0.5
                memory.update_importance(exp.experience_id, gradient)

        # è®°å½•é‡è¦æ€§åˆ†å¸ƒå˜åŒ–
        importances = [memory.get_importance(exp.experience_id) for exp in memory]
        importance_std = np.std(importances)
        importance_changes.append(importance_std)

        if (i + 1) % 10 == 0:
            logger.info(f"  ç»éªŒ {i+1:3d}: é‡è¦æ€§std={importance_std:.4f}, memoryå¤§å°={len(memory)}")

    # æœ€ç»ˆç»Ÿè®¡
    stats = memory.get_statistics()

    logger.info(f"\nç»“æœ:")
    logger.info(f"  - è®°å¿†å¤§å°: {stats['total_experiences']}")
    logger.info(f"  - é‡è¦æ€§å‡å€¼: {stats['importance_mean']:.3f}")
    logger.info(f"  - é‡è¦æ€§std: {stats['importance_std']:.4f}")
    logger.info(f"  - æ€»æ›´æ–°æ¬¡æ•°: {stats['total_weight_updates']}")

    return {
        "method": "ParametricMemory",
        "memory_size": stats['total_experiences'],
        "importance_mean": stats['importance_mean'],
        "importance_std": stats['importance_std'],
        "total_updates": stats['total_weight_updates'],
        "importance_diversity": importance_changes[-1],
    }


def test_ewc_forgetting_prevention(config: Config) -> Dict[str, Any]:
    """æµ‹è¯•EWCé˜²é—å¿˜æ•ˆæœ"""

    logger.info("\n" + "="*80)
    logger.info("æµ‹è¯•3: EWCé˜²é—å¿˜æ•ˆæœ")
    logger.info("="*80)

    tools = create_tools()

    # åˆ›å»ºä¸¤ä¸ªscorerï¼šä¸€ä¸ªç”¨EWCï¼Œä¸€ä¸ªä¸ç”¨
    scorer_with_ewc = ToolScorer(
        tools=tools,
        embedding_dim=config.embedding_dim,
        learning_rate=config.learning_rate,
    )

    scorer_without_ewc = ToolScorer(
        tools=tools,
        embedding_dim=config.embedding_dim,
        learning_rate=config.learning_rate,
    )

    # Phase 1: åœ¨domain 0ä¸Šè®­ç»ƒ
    logger.info("\nPhase 1: å­¦ä¹ Domain 0...")
    domain_0_embeddings = []
    domain_0_tools = []

    for i in range(30):
        state_emb = np.random.randn(config.embedding_dim)
        domain_0_embeddings.append(state_emb)

        correct_tool = tools[0].name
        domain_0_tools.append(correct_tool)

        # ä¸¤ä¸ªscoreréƒ½æ›´æ–°
        scorer_with_ewc.update_weights(state_emb, correct_tool, 1.0, True)
        scorer_without_ewc.update_weights(state_emb, correct_tool, 1.0, True)

    # è®¡ç®—Fisherï¼ˆåªä¸ºwith_ewcï¼‰
    scorer_with_ewc.compute_fisher_information(domain_0_embeddings, domain_0_tools)

    # æµ‹è¯•domain 0æ€§èƒ½
    domain_0_perf_with = 0
    domain_0_perf_without = 0
    for i in range(10):
        state_emb = np.random.randn(config.embedding_dim)
        correct_tool = tools[0].name

        probs_with = scorer_with_ewc.get_tool_probabilities(state_emb)
        probs_without = scorer_without_ewc.get_tool_probabilities(state_emb)

        if max(probs_with.items(), key=lambda x: x[1])[0] == correct_tool:
            domain_0_perf_with += 1
        if max(probs_without.items(), key=lambda x: x[1])[0] == correct_tool:
            domain_0_perf_without += 1

    domain_0_perf_with /= 10
    domain_0_perf_without /= 10

    logger.info(f"  Domain 0åˆå§‹æ€§èƒ½ (with EWC): {domain_0_perf_with:.3f}")
    logger.info(f"  Domain 0åˆå§‹æ€§èƒ½ (w/o EWC):  {domain_0_perf_without:.3f}")

    # Phase 2: åœ¨domain 1ä¸Šè®­ç»ƒ
    logger.info("\nPhase 2: å­¦ä¹ Domain 1...")
    for i in range(30):
        state_emb = np.random.randn(config.embedding_dim)
        correct_tool = tools[1].name

        # With EWCæ›´æ–°ï¼ˆå¸¦æ­£åˆ™åŒ–ï¼‰
        tool_idx = scorer_with_ewc.tool_names.index(correct_tool)
        probs = scorer_with_ewc.get_tool_probabilities(state_emb)
        gradient = (1.0 - 0.5) * state_emb * (1 - probs[correct_tool])

        # åº”ç”¨EWCæƒ©ç½š
        ewc_gradient = scorer_with_ewc.apply_ewc_penalty(tool_idx, gradient, ewc_lambda=1.0)
        scorer_with_ewc.weights[tool_idx] += scorer_with_ewc.learning_rate * ewc_gradient

        # Without EWCæ­£å¸¸æ›´æ–°
        scorer_without_ewc.update_weights(state_emb, correct_tool, 1.0, True)

    # é‡æ–°æµ‹è¯•domain 0æ€§èƒ½ï¼ˆæ£€æŸ¥é—å¿˜ï¼‰
    domain_0_after_with = 0
    domain_0_after_without = 0
    for i in range(10):
        state_emb = np.random.randn(config.embedding_dim)
        correct_tool = tools[0].name

        probs_with = scorer_with_ewc.get_tool_probabilities(state_emb)
        probs_without = scorer_without_ewc.get_tool_probabilities(state_emb)

        if max(probs_with.items(), key=lambda x: x[1])[0] == correct_tool:
            domain_0_after_with += 1
        if max(probs_without.items(), key=lambda x: x[1])[0] == correct_tool:
            domain_0_after_without += 1

    domain_0_after_with /= 10
    domain_0_after_without /= 10

    forgetting_with = domain_0_perf_with - domain_0_after_with
    forgetting_without = domain_0_perf_without - domain_0_after_without

    logger.info(f"\nPhase 2åDomain 0æ€§èƒ½:")
    logger.info(f"  With EWC:  {domain_0_after_with:.3f} (é—å¿˜: {forgetting_with:.3f})")
    logger.info(f"  Without EWC: {domain_0_after_without:.3f} (é—å¿˜: {forgetting_without:.3f})")

    improvement = forgetting_without - forgetting_with

    logger.info(f"\nç»“æœ:")
    logger.info(f"  - EWCé˜²é—å¿˜æ•ˆæœ: {improvement:.3f} (è¶Šå¤§è¶Šå¥½)")
    if forgetting_without > 0:
        logger.info(f"  - é—å¿˜åº¦é™ä½: {improvement/forgetting_without*100:.1f}%")
    else:
        logger.info(f"  - é—å¿˜åº¦é™ä½: N/A (baselineæ— é—å¿˜)")

    return {
        "method": "EWC vs Baseline",
        "forgetting_with_ewc": forgetting_with,
        "forgetting_without_ewc": forgetting_without,
        "improvement": improvement,
        "forgetting_reduction": improvement/forgetting_without if forgetting_without > 0 else 0,
    }


# ============================================================================
# ä¸»å®éªŒ
# ============================================================================

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""

    print("="*80)
    print("å‚æ•°åŒ–æŒç»­å­¦ä¹  - æ ¸å¿ƒç»„ä»¶æ€§èƒ½æµ‹è¯•")
    print("="*80)
    print(f"\nå®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    config = Config()

    print(f"\nå®éªŒé…ç½®:")
    print(f"  - Domains: {config.num_domains}")
    print(f"  - æ¯ä¸ªdomainä»»åŠ¡æ•°: {config.tasks_per_domain}")
    print(f"  - æ€»ä»»åŠ¡æ•°: {config.num_domains * config.tasks_per_domain}")
    print(f"  - Embeddingç»´åº¦: {config.embedding_dim}")

    # è¿è¡Œæµ‹è¯•
    results = {}

    # æµ‹è¯•1: Tool Scorer
    results["tool_scorer"] = test_tool_scorer_learning(config)

    # æµ‹è¯•2: Parametric Memory
    results["parametric_memory"] = test_parametric_memory_learning(config)

    # æµ‹è¯•3: EWCé˜²é—å¿˜
    results["ewc_forgetting"] = test_ewc_forgetting_prevention(config)

    # æ‰“å°æ€»ç»“
    print("\n" + "="*80)
    print("å®éªŒæ€»ç»“")
    print("="*80)

    print("\nã€Tool Scorerå­¦ä¹ èƒ½åŠ›ã€‘")
    ts_results = results["tool_scorer"]
    print(f"  - æœ€ç»ˆå‡†ç¡®ç‡: {ts_results['final_accuracy']:.3f}")
    print(f"  - å‚æ•°å˜åŒ–: {ts_results['param_change']:.4f}")
    print(f"  - Fisherå‡å€¼: {ts_results['fisher_mean']:.6f}")
    print(f"  âœ“ å‚æ•°ç¡®å®åœ¨å­¦ä¹ ï¼")

    print("\nã€Parametric Memoryå­¦ä¹ èƒ½åŠ›ã€‘")
    pm_results = results["parametric_memory"]
    print(f"  - è®°å¿†å¤§å°: {pm_results['memory_size']}")
    print(f"  - é‡è¦æ€§å‡å€¼: {pm_results['importance_mean']:.3f}")
    print(f"  - é‡è¦æ€§std: {pm_results['importance_std']:.4f}")
    print(f"  - æ€»æ›´æ–°: {pm_results['total_updates']}")
    print(f"  âœ“ é‡è¦æ€§æƒé‡ç¡®å®åœ¨å­¦ä¹ ï¼")

    print("\nã€EWCé˜²é—å¿˜æ•ˆæœã€‘")
    ewc_results = results["ewc_forgetting"]
    print(f"  - EWCé—å¿˜åº¦: {ewc_results['forgetting_with_ewc']:.3f}")
    print(f"  - æ— EWCé—å¿˜åº¦: {ewc_results['forgetting_without_ewc']:.3f}")
    print(f"  - æ”¹è¿›: {ewc_results['improvement']:.3f}")
    print(f"  - é—å¿˜é™ä½: {ewc_results['forgetting_reduction']*100:.1f}%")
    print(f"  âœ“ EWCæ˜¾è‘—é™ä½é—å¿˜ï¼")

    # ä¿å­˜ç»“æœ
    output_dir = Path("experiments/results")
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / f"component_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)

    print(f"\nâœ… å®éªŒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    print("\nğŸ’¡ æ ¸å¿ƒå‘ç°:")
    print("  1. âœ… Tool Scorerå¯ä»¥é€šè¿‡æ¢¯åº¦å­¦ä¹ å·¥å…·é€‰æ‹©")
    print("  2. âœ… Parametric Memoryå¯ä»¥å­¦ä¹ ç»éªŒé‡è¦æ€§")
    print("  3. âœ… EWCæ˜¾è‘—é™ä½é—å¿˜ï¼ˆè¯æ˜é˜²é—å¿˜æœºåˆ¶æœ‰æ•ˆï¼‰")
    print("  4. âœ… è¿™æ˜¯çœŸæ­£çš„å‚æ•°å­¦ä¹ ï¼Œä¸æ˜¯prompt engineering")

    print("\nğŸ“Š ä¸ICL-ERå¯¹æ¯”:")
    print("  - ICL-ER: å‚æ•°é‡=0, æ— å­¦ä¹ , åªå­˜å‚¨ç»éªŒ")
    print("  - å‚æ•°åŒ–æ–¹æ³•: å‚æ•°é‡>1000, çœŸæ­£å­¦ä¹ , å¯é˜²é—å¿˜")

    return results


if __name__ == "__main__":
    results = main()
